{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunny-5555/Computer-Vision-Assignments/blob/add-notebooks/a2-convolution-panoramaStitching/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS7T20gWaEkg"
      },
      "source": [
        "# Machine Vision - Assignment 2: Convolution, Cartoonization and Panorama Stitching\n",
        "\n",
        "---\n",
        "\n",
        "Prof. Dr. Markus Enzweiler, Esslingen University of Applied Sciences\n",
        "\n",
        "markus.enzweiler@hs-esslingen.de\n",
        "\n",
        "---\n",
        "\n",
        "This is the second assignment for the \"Machine Vision\" lecture. \n",
        "It covers:\n",
        "* implementing your own image convolution function\n",
        "* applying OpenCV functions for Canny edge detection, bilateratal filtering, cartoonization\n",
        "* implementing an image panorama stitching function based on SIFT and RANSAC\n",
        "\n",
        "To successfully complete this assignment, it is assumed that you already have some experience in Python and numpy. You can either use [Google Colab](https://colab.research.google.com/) for free with a private (dedicated) Google account (recommended) or a local Jupyter installation.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH3vu3HiqaZS"
      },
      "source": [
        "## Preparations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFz0s31SxNyP"
      },
      "source": [
        "### Import important libraries (you should probably start with these lines all the time ...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPcN62DgZ6Gg"
      },
      "source": [
        "# OpenCV\n",
        "import cv2   \n",
        "\n",
        "# NumPy                    \n",
        "import numpy as np     \n",
        "\n",
        "# Matplotlib    \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "# make sure we show all plots directly below each cell\n",
        "%matplotlib inline \n",
        "\n",
        "# Request\n",
        "import requests\n",
        "\n",
        "# Some Colab specific packages\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  # image display\n",
        "  from google.colab.patches import cv2_imshow "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANRNsyDPTm9x"
      },
      "source": [
        "\n",
        "### Some helper functions that we will need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ5AIQr1TsHU"
      },
      "source": [
        "def my_imshow(image, windowTitle=\"Image\"):\n",
        "  '''\n",
        "  Displays an image and differentiates between Google Colab and a local Python installation. \n",
        "\n",
        "  Args: \n",
        "    image: The image to be displayed\n",
        "\n",
        "  Returns:\n",
        "    - \n",
        "  '''\n",
        "\n",
        "  if 'google.colab' in str(get_ipython()):\n",
        "    cv2_imshow(image)\n",
        "  else:\n",
        "    cv2.imshow(windowTitle, image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQJnwTJQPzvF"
      },
      "source": [
        "## Exercise 1 - Image Convolution (10 points) \n",
        "\n",
        "*Note: This is approx. 20-25 lines of Python code to implement.*\n",
        "\n",
        "Implement a convolution function ```myConvolution(inImage, kernel)``` that takes in as input an arbitrary image and convolution kernel. Use only your own code, i.e. no OpenCV or other external libraries providing a dedicated convolution function. At the boundaries of the image, assume that the values outside the image are zero. A good strategy to do so is to pad the image with zeros prior to convolution  Padding involves adding additional rows and columns containing zeros at the image boundaries and removing the padding after convolution. \n",
        "\n",
        "Test your function with the provided image and kernel. Your output should look identical (maybe minor differences at the image boundaries due to different border handling) to the OpenCV reference ```cv2.filter2D()```that is provided. How does your implementation compare to the OpenCV implementation regarding computation time? You can use ```%%time``` for timing measurements. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bhV8Ca6r26f"
      },
      "source": [
        "---\n",
        "\n",
        "The following code contains a code template as well as an OpenCV implementation of image convolution as reference. Please add your own implementation to the function ```myConvolution(inImage, kernel)```. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl29rs1LtUfK"
      },
      "source": [
        "### Load and display the input image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yas3Z1jPzvG"
      },
      "source": [
        "# Load and display the input image \n",
        "\n",
        "url = 'https://raw.githubusercontent.com/sunny-5555/Computer-Vision-Assignments/main/a2-convolution-panoramaStitching/convolution/esslingen800px.jpg'\n",
        "response = requests.get(url, allow_redirects = True, stream = True).raw\n",
        "image = np.asarray(bytearray(response.read()), dtype=\"uint8\")\n",
        "\n",
        "# read image and convert to gray\n",
        "inputImage = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "inputImage = cv2.cvtColor(inputImage, cv2.COLOR_BGRA2GRAY)\n",
        "\n",
        "# Display the input image\n",
        "print(\"Input image\")\n",
        "my_imshow(inputImage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz1uQTbAtd3k"
      },
      "source": [
        "### Create a Gaussian filter kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vQhWw39tj-j"
      },
      "source": [
        "# Create a Gaussian filter kernel\n",
        "kernel = 1.0/16.0 * np.array([[1,2,1], [2,4,2], [1,2,1]])\n",
        "print(\"Gaussian Kernel\\n{}\".format(kernel))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feIA-pc1tnaa"
      },
      "source": [
        "### Apply the filter using OpenCV's ```filter2D()``` function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_6WrcMptu8w"
      },
      "source": [
        "%%time\n",
        "outputImage = cv2.filter2D(inputImage, -1, kernel, borderType=cv2.BORDER_CONSTANT)\n",
        "print(\"OpenCV filter2D() convolution result\")\n",
        "my_imshow(outputImage)\n",
        "print(outputImage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFKvwPq6uM9s"
      },
      "source": [
        "### Implement your own convolution function ```myConvolution()``` (**add your code here**)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLFh832ZuZWY"
      },
      "source": [
        "# Template for your own convolution function\n",
        "\n",
        "def myConvolution(inImage, kernel):\n",
        "  '''\n",
        "  2D image convolution of an image with a filter kernel.\n",
        "\n",
        "  Args:   \n",
        "    inImage: input image\n",
        "    kernel: input 2D filter kernel\n",
        "\n",
        "  Returns:\n",
        "    outImage: output image, the input image convolved with the filter kernel\n",
        "  '''\n",
        "\n",
        "  padding_border_width = 1\n",
        "  padding_value = 0\n",
        "  inImagePad = np.pad(inImage, padding_border_width, 'constant', constant_values=padding_value)\n",
        "\n",
        "  avg_win_size = 3\n",
        "  outImage = np.copy(inImage)\n",
        "\n",
        "  for x in range(0,len(inImage)):\n",
        "    for y in range(0,len(inImage[x])):\n",
        "      averaging_window = np.empty((0,avg_win_size), int)\n",
        "      for x_avg in range(0,avg_win_size):\n",
        "        coppied_arr = np.copy(inImagePad[x+x_avg][y:y+avg_win_size])\n",
        "        averaging_window = np.append(averaging_window, [coppied_arr],axis=0)\n",
        "      products = np.multiply(kernel, averaging_window)\n",
        "      convoluted_el = products.sum()\n",
        "      outImage[x,y] = convoluted_el\n",
        "\n",
        "  return outImage\n",
        "outputImage = myConvolution(inputImage, kernel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKXMszNrvOV-"
      },
      "source": [
        "### Apply and time your own convolution function \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKH_nafFvVbg"
      },
      "source": [
        "# Run your own convolution function\n",
        "%%time\n",
        "myoutputImage = myConvolution(inputImage, kernel)\n",
        "print(\"myConvolution() convolution result\")\n",
        "my_imshow(outputImage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71vxUHg29qkd"
      },
      "source": [
        "rows = len(outputImage)\n",
        "for row in range(0,rows-1):\n",
        "  for n in range(0,len(outputImage[row])-1):\n",
        "    if outputImage[row][n] != myoutputImage[row][n]:\n",
        "      print(f\"Element {row},{n} not equal\\noutputImage[row][n] = {outputImage[row][n]}\\nmyoutputImage[row][n] = {myoutputImage[row][n]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6iVCqac-D_Q"
      },
      "source": [
        "## Exercise 2 - Canny Edge Detection and Cartoonization with OpenCV (10 points)\n",
        "\n",
        "*Note: This is approx. 10 lines of Python code to implement.*\n",
        "\n",
        "1.   Use your favorite search engine to find out how to do run the Canny Edge Detector in OpenCV, as we have seen in the lecture. Use it to detect edges in the provided image ```ironman.jpg``` and visualize the result. \n",
        "\n",
        "2.   Use your favorite search engine to find out how to do to cartoonization in OpenCV as a combination of edge detection and bilateral filtering, as we have seen in the lecture. Apply it to the image ```ironman.jpg``` and visualize the result. \n",
        "\n",
        "Your results should look similar to the following images (see ```result.jpg```). You can right-click on this image and \"open in new tab/window\" to display it in its original size:\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=16Zb5fsUfaGWi0YI39pezJ8vu-3UlWED4'></img>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oeCSJaRB0ZL"
      },
      "source": [
        "# get image\n",
        "url = 'https://raw.githubusercontent.com/sunny-5555/Computer-Vision-Assignments/main/a2-convolution-panoramaStitching/canny_cartoonization/ironman.jpg'\n",
        "response = requests.get(url, allow_redirects = True, stream = True).raw\n",
        "image = np.asarray(bytearray(response.read()), dtype=\"uint8\")\n",
        "\n",
        "# read image\n",
        "inputImage = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "\n",
        "# Display the input image\n",
        "print(\"Input image\")\n",
        "my_imshow(inputImage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkmOfr_359Rw"
      },
      "source": [
        "### Canny Edge Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hao9TsQzBA9A"
      },
      "source": [
        "cannyImage = cv2.Canny(inputImage,110,200)\n",
        "print(\"Canny edge detection image\")\n",
        "cv2_imshow(cannyImage)\n",
        "print(cannyImage)\n",
        "print(cannyImage.shape)\n",
        "print(inputImage.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLZXWhyW6L7F"
      },
      "source": [
        "### Cartoonization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dxffv5ACRmn"
      },
      "source": [
        "inImage_bil_filter = cv2.bilateralFilter(inputImage,15,80,80) #bilateral_blur = cv2.bilateralFilter(img,15,80,80)  9,75,75\n",
        "cannyImage = cv2.Canny(inImage_bil_filter,110,200)\n",
        "print(\"Canny edge detection image\")\n",
        "cv2_imshow(cannyImage)\n",
        "print(cannyImage)\n",
        "cartoon = cv2.subtract(inImage_bil_filter, cv2.cvtColor(cannyImage, cv2.COLOR_GRAY2BGR))\n",
        "my_imshow(cartoon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IxxAKTfxxiU"
      },
      "source": [
        "## Exercise 3 - Panorama Stitching with SIFT and RANSAC (10 points)\n",
        "\n",
        "*Note: This is approx. 30-40 lines of Python code to implement.*\n",
        "\n",
        "In this exercise you will work on image panorama stitching using SIFT and RANSAC. In principle, this exercise follows the exact pipeline that we have discussed in the lecture:\n",
        "\n",
        "\n",
        "1.   Compute SIFT feature descriptors for both input images (**provided, not to be implemented**).\n",
        "2.   Compute distances between every SIFT descriptor in one image and every descriptor in the other image (**provided, not to be implemented**).\n",
        "3. Select closest matches based on the matrix of pairwise descriptor distances obtained above. We will select the top 2500 descriptor pairs with the smallest pairwise distances (**provided, not to be implemented**).\n",
        "4. Implement a RANSAC-based estimation of the homography between the two provided images (***to be implemented***) that is can be used to stitch the two images into a single panoramic image. An OpenCV reference implementation is provided for you to compare against. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdfZDFJENEYr"
      },
      "source": [
        "### Preparations and provided functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFzeGgygOeX8"
      },
      "source": [
        "#### Packages, Imports and Drive Mounting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUBZvB_CNU9V"
      },
      "source": [
        "# We will need to install a different opencv package and restart the Colab runtime, see below. \n",
        "# All the imports are reproduced here again so that you do not need to run anything from the \n",
        "# previous two exercises after restarting the Colab runtime. \n",
        "\n",
        "\n",
        "# Imports \n",
        "\n",
        "import sys\n",
        "import random\n",
        "\n",
        "# OpenCV\n",
        "import cv2   \n",
        "\n",
        "# NumPy                    \n",
        "import numpy as np     \n",
        "\n",
        "# Matplotlib    \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "# make sure we show all plots directly below each cell\n",
        "%matplotlib inline \n",
        "\n",
        "# Some Colab specific packages\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  # image display\n",
        "  from google.colab.patches import cv2_imshow \n",
        "\n",
        "\n",
        "\n",
        "# For SIFT, we need to install a more recent opencv version that what is \n",
        "# provided in the Colab runtime :(\n",
        "    \n",
        "OpenCVVersion = \"4.5.1.48\" # which version shall be installed?\n",
        "\n",
        "if 'google.colab' in sys.modules and (cv2.__version__ not in OpenCVVersion) :\n",
        "    import subprocess\n",
        "    print(\"Installing opencv-contrib-python version {}\".format(OpenCVVersion))\n",
        "    !pip uninstall opencv-python -y\n",
        "    !pip install opencv-contrib-python==$OpenCVVersion\n",
        "    print('Please manually restart Colab runtime ...')\n",
        "   \n",
        "print(\"Installed OpenCV version is {}\".format(cv2.__version__))\n",
        "\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive', force_remount=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dipbUhbOk8N"
      },
      "source": [
        "#### Provided Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOrUmMD7OnGX"
      },
      "source": [
        "# Display an image\n",
        "def my_imshow(image, windowTitle=\"Image\"):\n",
        "  '''\n",
        "  Displays an image and differentiates between Google Colab and a local Python installation. \n",
        "\n",
        "  Args: \n",
        "    image: The image to be displayed\n",
        "\n",
        "  Returns:\n",
        "    - \n",
        "  '''\n",
        "\n",
        "  if 'google.colab' in str(get_ipython()):\n",
        "    cv2_imshow(image)\n",
        "  else:\n",
        "    cv2.imshow(windowTitle, image)\n",
        "\n",
        "\n",
        "\n",
        "# Provided function to compute an homography between two sets of points p1 and p2 based on least squares\n",
        "# Input format is the same as for cv2.findHomography()\n",
        "def myComputeHomography(p1, p2):\n",
        "  '''\n",
        "  Estimates an homography between two sets of points p1 and p2 based on least squares.  \n",
        "  Input format is the same as for cv2.findHomography(). At least 4 input points are needed in each input set. \n",
        "  '''\n",
        "  A = []\n",
        "  for i in range(0, len(p1)):\n",
        "      x, y = p1[i][0][0], p1[i][0][1]\n",
        "      u, v = p2[i][0][0], p2[i][0][1]\n",
        "      A.append([x, y, 1, 0, 0, 0, -u*x, -u*y, -u])\n",
        "      A.append([0, 0, 0, x, y, 1, -v*x, -v*y, -v])\n",
        "  A = np.asarray(A)\n",
        "  U, S, Vh = np.linalg.svd(A)\n",
        "  L = Vh[-1,:] / Vh[-1,-1]\n",
        "  return L.reshape(3, 3)\n",
        "\n",
        "\n",
        "\n",
        "# Provided function to stitch two images together using the provided homography\n",
        "def myStitchImages(imageLeft, imageRight, homography):\n",
        "  '''\n",
        "  Stitch two images together using the provided homography\n",
        "  '''\n",
        "  # Reserve some space for the final stitched image\n",
        "  stitchedImageHeight = imageLeft.shape[0] + imageRight.shape[0]\n",
        "  stitchedImageWidth  = imageLeft.shape[1] + imageRight.shape[1]\n",
        "\n",
        "  # Warp the right image \n",
        "  stitchedImage = cv2.warpPerspective(imageRight, homography, dsize = (stitchedImageWidth, stitchedImageHeight))\n",
        "\n",
        "  # Copy the left image into the stitched image\n",
        "  stitchedImage[0:imageLeft.shape[0], 0:imageLeft.shape[1], :] = imageLeft\n",
        "\n",
        "  return stitchedImage\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATTSTJH6P8ZX"
      },
      "source": [
        "### Compute and visualize SIFT keypoints and descriptors for both input images (**provided**) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrPJ4BVfQFIV"
      },
      "source": [
        "# Load the input images and convert to gray\n",
        "\n",
        "url_base = 'https://raw.githubusercontent.com/sunny-5555/Computer-Vision-Assignments/main/a2-convolution-panoramaStitching/panoramaStitching/'\n",
        "\n",
        "response = requests.get(url_base + \"esslingenPanoLeft.jpg\", allow_redirects = True, stream = True).raw\n",
        "image = np.asarray(bytearray(response.read()), dtype=\"uint8\")\n",
        "inputImageLeftColor = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "inputImageLeft      = cv2.cvtColor(inputImageLeftColor, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "response = requests.get(url_base + \"esslingenPanoRight.jpg\", allow_redirects = True, stream = True).raw\n",
        "image = np.asarray(bytearray(response.read()), dtype=\"uint8\")\n",
        "inputImageRightColor = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "inputImageRight      = cv2.cvtColor(inputImageRightColor, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "\n",
        "\n",
        "# Following should be uncommented if the images on Google Drive shall be used\n",
        "# imPath = \"/content/drive/My Drive/Machine Vision/MV_assignments/a2-convolution-panoramaStitching/panoramaStitching/\"\n",
        "\n",
        "# inputImageLeftColor = cv2.imread(imPath + \"esslingenPanoLeft.jpg\")\n",
        "# inputImageLeft      = cv2.cvtColor(inputImageLeftColor, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# inputImageRightColor = cv2.imread(imPath + \"esslingenPanoRight.jpg\")\n",
        "# inputImageRight      = cv2.cvtColor(inputImageRightColor, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Display the input images\n",
        "print(\"Input image (left)\")\n",
        "my_imshow(inputImageLeftColor)\n",
        "\n",
        "print(\"Input image (right)\")\n",
        "my_imshow(inputImageRightColor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHdwb2uVQrfN"
      },
      "source": [
        "# Compute SIFT features for both the left and right image\n",
        "\n",
        "# Left image\n",
        "# Apply SIFT feature detector \n",
        "sift = cv2.SIFT_create()\n",
        "keyPointsLeft, descriptorsLeft = sift.detectAndCompute(inputImageLeft,None)\n",
        "siftImageLeft = cv2.drawKeypoints(inputImageLeft, keyPointsLeft, inputImageLeft, \\\n",
        "                                  flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "\n",
        "# Display image\n",
        "print(\"Found {} {}-dimensional SIFT Features (left)\".format(descriptorsLeft.shape[0], descriptorsLeft.shape[1]))\n",
        "my_imshow(siftImageLeft)\n",
        "\n",
        "# Right image\n",
        "# Apply SIFT feature detector \n",
        "sift = cv2.SIFT_create()\n",
        "keyPointsRight, descriptorsRight = sift.detectAndCompute(inputImageRight,None)\n",
        "siftImageRight = cv2.drawKeypoints(inputImageRight, keyPointsRight, inputImageRight, \\\n",
        "                                   flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "\n",
        "# Display image\n",
        "print(\"Found {} {}-dimensional SIFT Features (right)\".format(descriptorsRight.shape[0], descriptorsRight.shape[1]))\n",
        "my_imshow(siftImageRight)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5P7r_rHQ9hm"
      },
      "source": [
        "### Compute distances between every SIFT descriptor in one image and every descriptor in the other image. Select the top 2500 matches (**provided**)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsVNpyKvRAUC"
      },
      "source": [
        "# Match SIFT descriptors between two images\n",
        "# Brute force matching: For every SIFT descriptor in the left image, the best matching \n",
        "# (= most similar SIFT descriptor) in the right image is found.\n",
        "# See: OpenCV BFMatcher class reference\n",
        "\n",
        "matcher = cv2.BFMatcher(normType = cv2.NORM_L2)\n",
        "matches = matcher.match(descriptorsLeft, descriptorsRight) \n",
        "print(\"# of matches: {}\".format(len(matches)))\n",
        "\n",
        "\n",
        "# Only take the best \"n\" matches, i.e. the ones with the smallest distance between SIFT descriptor vectors\n",
        "\n",
        "numMatchesUsed = 2500\n",
        "# Sort them in the order of their distance and only take \"n\"\n",
        "matches = sorted(matches, key = lambda x:x.distance)[:numMatchesUsed]\n",
        "\n",
        "# Extract the locations of matched keypoints in both images\n",
        "locationsLeft  = np.float32( [ keyPointsLeft  [match.queryIdx].pt for match in matches]).reshape(-1,1,2)\n",
        "locationsRight = np.float32( [ keyPointsRight [match.trainIdx].pt for match in matches]).reshape(-1,1,2)\n",
        "\n",
        "print(\"# of selected matches: {}\".format(len(locationsLeft)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qrwavtq-Re_J"
      },
      "source": [
        "### OpenCV reference implementation of RANSAC-based homography estimation and stitching (**provided for reference**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsa77qfRRrDv"
      },
      "source": [
        "# RANSAC parameters (for both OpenCV and your implementation)\n",
        "\n",
        "rNumTrials = 250\n",
        "rMatchThresh = 7.0 # in pixels\n",
        "\n",
        "# OpenCV computed homography for reference\n",
        "openCVHomography, mask = cv2.findHomography(locationsRight, locationsLeft, cv2.RANSAC, \\\n",
        "                             ransacReprojThreshold = rMatchThresh, maxIters = rNumTrials)\n",
        "print(\"OpenCV computed homography (for reference) is: \\n{}\". format(openCVHomography))\n",
        "\n",
        "\n",
        "# Stitch\n",
        "stitchedImage = myStitchImages(inputImageLeftColor, inputImageRightColor, openCVHomography)\n",
        "\n",
        "print(\"Result using OpenCV reference implementation\")\n",
        "my_imshow(stitchedImage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzhYCKJYTiIb"
      },
      "source": [
        "### Your own RANSAC-based homography estimation and stitching (**to be implemented**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILGixiytTo2A"
      },
      "source": [
        "# RANSAC parameters\n",
        "rNumTrials = 250\n",
        "rMatchThresh = 7.0 # in pixels\n",
        "\n",
        "\n",
        "# variables\n",
        "inlierFraction = 0\n",
        "rBestInlierFraction = 0 \n",
        "rBestTrial = 0\n",
        "num_inliers = 0\n",
        "\n",
        "# a dummy homography\n",
        "rBestHomography =  np.identity(3)\n",
        "rBestHomography[0,2] = 1500\n",
        "\n",
        "# Main RANSAC loop\n",
        "print(\"Starting RANSAC loop with {} iterations\".format(rNumTrials))\n",
        "\n",
        "for iTrial in range(1, rNumTrials):\n",
        "  # 1) Select 4 random matches out of the 2500. The actual point coordinates are in the \"keyPointsLeft\"\n",
        "  #    and \"keyPointsRight\" array. See how the 2500 matches were selected out of the full set. Now, we need\n",
        "  #    4 out of 2500 as 4 is the minimum number of points that is needed to compute an homography.\n",
        "  \n",
        "  \n",
        "  idxes = np.random.randint(0, numMatchesUsed-1, 4)\n",
        "  randomMatchesLeft = []\n",
        "  randomMatchesRight = []\n",
        "  for idx in idxes:\n",
        "    randomMatchesLeft.append(locationsLeft[idx])\n",
        "    randomMatchesRight.append(locationsRight[idx])\n",
        "\n",
        "  # 2) Compute the homography using the provided myComputeHomography() function, see above. \n",
        "  \n",
        "  homographieRandomMatches = myComputeHomography(randomMatchesRight,randomMatchesLeft)\n",
        "\n",
        "  # 3) Evaluate quality of the homography by computing the fraction of inliers through all 2500 matches. \n",
        "  #    Inliers are defined as the number of warped points from the right image (using the homography) that lie\n",
        "  #    within a user-defined distance \"rMatchThresh\" pixels of their corresponding point in image left image as \n",
        "  #    defined by the SIFT matching. \n",
        "\n",
        "  num_inliers = 0  \n",
        "  # for all 2500 matches\n",
        "  for iMatch in range(0, len(matches)):    \n",
        "    ;\n",
        "    # Get the coordinates of left and right points for the current \"match\"\n",
        "    # (in homogeneous coordinates, i.e. a 3-dimensional vector with an added \"1.0\" as third element)\n",
        "  \n",
        "    leftPt = locationsLeft[iMatch]\n",
        "    left3dVector = np.array([leftPt[0][0],leftPt[0][1],1.0]).reshape((1,3))\n",
        "    rightPt = locationsRight[iMatch]\n",
        "    right3dVector = np.array([rightPt[0][0],rightPt[0][1],1.0]).reshape((1,3))\n",
        "    # Warp the right point to the left image using the homography. \n",
        "    # Convert back from homogenous coordinates by dividing the first and second element of the vector by the third\n",
        "   \n",
        "    warpRight = np.matmul(homographieRandomMatches,right3dVector.T)\n",
        "    right2dWarp = np.array([warpRight[0],warpRight[1]])\n",
        "    right2dWarp = 1/warpRight[2] * right2dWarp\n",
        "\n",
        "    # Compute euclidean distance between the original left point and the right point warped to the left image\n",
        "   \n",
        "    dist = np.linalg.norm(leftPt-right2dWarp.T)\n",
        "\n",
        "    # Inlier or outlier based on the euclidean distance and rMatchThresh\n",
        "    \n",
        "    if dist < rMatchThresh:\n",
        "      num_inliers += 1\n",
        "\n",
        "  # --------- loop over all matches ends here --------------\n",
        "\n",
        "\n",
        "  # 4) Use the fraction of inliers (num inliers / all matches) to evaluate the estimated homography. \n",
        "  #    If it is better than a previous one, store it. \n",
        "    \n",
        "  # 5) Some status output\n",
        "\n",
        "  inlierFraction = num_inliers / len(matches)\n",
        "\n",
        "  if rBestInlierFraction < inlierFraction:\n",
        "    rBestInlierFraction = inlierFraction\n",
        "    rBestHomography = homographieRandomMatches\n",
        "    rBestTrial = iTrial\n",
        "\n",
        "  print(\"---------------------------------------------------------------------------------------\") \n",
        "  print(\"RANSAC iteration [{}] : Found transform with {:0.2f}% inliers\".format(iTrial, inlierFraction * 100))\n",
        "  print(\"Current best transform is in iteration {} with {:0.2f}% inliers\".format(rBestTrial, rBestInlierFraction * 100))\n",
        "\n",
        "# --------- end of a single RANSAC iteration --------------\n",
        "\n",
        "\n",
        "\n",
        "# 6) RANSAC loop (all iterations) completed\n",
        "\n",
        "print(\"---------------------------------------------------------------------------------------\") \n",
        "print(\"RANSAC loop completed\")\n",
        "print(\"Best transform is in iteration {} with {:0.2f}% inliers\".format(rBestTrial, rBestInlierFraction * 100))\n",
        "print(\"Estimated Homography: \\n{}\".format(rBestHomography))\n",
        "\n",
        "# Stitch using the best estimated homography\n",
        "stitchedImage = myStitchImages(inputImageLeftColor, inputImageRightColor, rBestHomography)\n",
        "\n",
        "print(\"Result using implemented RANSAC\")\n",
        "my_imshow(stitchedImage)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}